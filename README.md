# GPU Doctor

*A lightweight telemetry + LLM assistant that slashes VRAM waste and Out-Of-Memory crashes on shared NVIDIA GPU servers.*

---

## âœ¨ What it does

* Collects `nvidia-smi` / DCGM telemetry every *N* seconds and stores it in SQLite.
* Generates MiniLM embeddings of each log line for semantic search.
* Exposes a FastAPI endpoint **`POST /ask_gpu`** that retrieves the most relevant logs, feeds them to an LLM (OpenAI o3 by default) and returns a JSON diagnosis with resource recommendations.
* Optional wrapper CLI `gpu_doc run â€¦` tags your jobs automatically so queries like â€œWhy did **run-42** thrash?â€ are pinpoint-accurate.

---

## ğŸ—‚ï¸ Repo layout

```
gpu_doctor/
â”‚  setup.cfg              â† packaging info + console-scripts
â”‚  README.md
â”‚
â”œâ”€ gpu_doctor/
â”‚   â”œâ”€ api.py             â† FastAPI (ask_gpu)
â”‚   â”œâ”€ cli.py             â† gpu_doc run â€¦
â”‚   â””â”€ collector/
â”‚        â”œâ”€ poller.py     â† telemetry daemon
â”‚        â”œâ”€ db.py
â”‚        â”œâ”€ embeddings.py
â”‚        â””â”€ retriever.py
â”‚
â”œâ”€ deploy/
â”‚   â”œâ”€ gpu-doctor.service â† example systemd unit
â”‚   â””â”€ docker-compose.yaml    â† GPU-enabled container
â””â”€ scripts/
    â””â”€ backfill_embeddings.py
```

---

## âš¡ Quick start

### Windows workstation (no GPU required)

```powershell
git clone https://github.com/<you>/gpu_doctor.git
cd gpu_doctor
python -m venv .venv; .\.venv\Scripts\Activate
pip install -r requirements-linux.txt
pip install -e .

python -m collector.poller --once          # one snapshot â†’ gpu_logs.db
python scripts/backfill_embeddings.py      # builds FAISS index
uvicorn gpu_doctor.api:app --port 8080     # RAG endpoint at http://localhost:8080
```

### Ubuntu 22.04 GPU server

```bash
git clone https://github.com/<you>/gpu_doctor.git && cd gpu_doctor
python3.8 -m venv .venv && source .venv/bin/activate
pip install -r requirements-linux.txt && pip install -e .

sudo cp deploy/gpu-doctor.service /etc/systemd/system/
sudo systemctl daemon-reload && sudo systemctl enable --now gpu-doctor
```

### Docker (NVIDIA Container Runtime required)

```bash
docker compose up -d                     # API on port 8080, data in ./data
```

---

## ğŸ”§ Common CLI

| Task               | Command                                                             |
| ------------------ | ------------------------------------------------------------------- |
| Tag & launch a job | `gpu_doc run python train.py`                                       |
| Manual snapshot    | `gpu_poll --once` *(alias for `python -m collector.poller --once`)* |
| Ask â€œwhyâ€ via curl | `curl -X POST localhost:8080/ask_gpu -d '{"run_id":"run-42"}'`      |

---

## ğŸ› ï¸ Environment knobs

| Variable            | Default     | Purpose                                         |
| ------------------- | ----------- | ----------------------------------------------- |
| `GPU_DOC_POLL_SEC`  | `30`        | Polling interval (seconds)                      |
| `GPU_DOC_KEEP_DAYS` | `7`         | Retention window for auto-prune                 |
| `GPU_DOC_TOPK`      | `8`         | How many log chunks to send to the LLM          |
| `GPU_DOC_MODEL`     | `openai:o3` | Model alias (`openai:o3`, `llama3:local`, etc.) |
| `OPENAI_API_KEY`    | â€”           | Required only for OpenAI endpoints              |

---

## ğŸ“ˆ Roadmap

* DCGM metrics (ECC, throttling) enrichment
* Slack / Teams bot wrapper
* Grafana dashboard with per-run timelines
* K8s & Slurm deployment charts

